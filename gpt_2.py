# -*- coding: utf-8 -*-
"""gpt-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BWhAcYKueQQAkqgg946rCfSroik2uvKF
"""

import torch

if torch.cuda.is_available():
    print("GPU is available")
else:
    print("GPU is not available")

! pip install transformers
import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel

epoch = 30 # 20
batch_size =50
max_len= 80
lr = 0.00005 # 0.0001

from zmq import device
device = 'cuda'
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
tokenizer.add_special_tokens({'additional_special_tokens': ['[EOS]', '[BOS]']})
model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
model.resize_token_embeddings(len(tokenizer))
for module in model.modules():
    if isinstance(module, nn.Dropout):
        module.p = 0.5

from google.colab import drive
drive.mount('/content/drive')

path="normalized_emotion_conv_total_train_dataset.txt"
with open(path, encoding='utf-8') as dataset_file:
    text = dataset_file.readlines()
import random
random.shuffle(text)
print(len(text))
print(text[0])

# loader_text = DataLoader(text, batch_size=batch_size)

optimizer = torch.optim.Adam(model.parameters(), lr=lr)#, weight_decay=0.01) # add L2 regularizer

def trainer_classifier():
    patience = 0
    curr_ppl = 0
    for epoch_val in range(epoch):
      random.shuffle(text)
      loader_text = DataLoader(text, batch_size=batch_size)
      nllls = []
      prev_ppl = curr_ppl
      batch_indx = 0
      for data_batch in loader_text:
        batch = tokenizer.batch_encode_plus(data_batch, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=True,return_tensors = 'pt')#, batch_first = True)
        input_ids = batch['input_ids'].to(device)
        target = input_ids.clone()
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        if batch_indx == 0:
          total_attention_mask = attention_mask
          total_token_type_ids = token_type_ids
        else:
          total_attention_mask = torch.concatenate([total_attention_mask,attention_mask],dim=0)
          total_token_type_ids = torch.concatenate([total_token_type_ids,token_type_ids],dim=0)
        outputs = model(input_ids, labels=target)
        negative_log_likelihood = outputs.loss
        negative_log_likelihood.backward()
        nllls.append(negative_log_likelihood)
        optimizer.step()
        optimizer.zero_grad()
        batch_indx +=1
      ppl = torch.exp(torch.stack(nllls).mean())
      curr_ppl = ppl
      if curr_ppl >= prev_ppl:  # Implementing Early Stopping
          patience += 1
          if patience >1:
              print(f'ppl for epoch {epoch_val +1} is {ppl}')
              print("Model training procedure is finished earlier due to early stopping.")
              model.save_pretrained('saved_gpt2_model_new')
              tokenizer.save_pretrained('saved_gpt2_model_new')
              attention_mask = total_attention_mask
              torch.save(attention_mask, 'saved_gpt2_model_new/attention_mask.pt')
              token_type_ids = total_token_type_ids
              torch.save( token_type_ids, 'saved_gpt2_model_new/token_type_ids.pt')
              return
          else:
              print(f'ppl for epoch {epoch_val +1} is {ppl}')
      else:
          patience = 0
          print(f'ppl for epoch {epoch_val +1} is {ppl}')

    model.save_pretrained('saved_gpt2_model_new')
    tokenizer.save_pretrained('saved_gpt2_model_new')
    attention_mask = total_attention_mask
    torch.save(attention_mask, 'saved_gpt2_model_new/attention_mask.pt')
    token_type_ids = total_token_type_ids
    torch.save( token_type_ids, 'saved_gpt2_model_new/token_type_ids.pt')

trainer_classifier()

#! zip -r saved_gpt2_model_new.zip saved_gpt2_model_new

# from google.colab import files

# files.download('saved_gpt2_model_new.zip')
